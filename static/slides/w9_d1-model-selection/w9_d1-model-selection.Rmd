---
title: "Model selection <br> `r emo::ji('pick')`"
author: "Dr. Ã‡etinkaya-Rundel"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(here)
library(DT)
library(modelr)
```

```{r load-pp, include=FALSE}
pp <- read_csv(here::here("csv/paris-paintings.csv"), na = c("n/a", "", "NA"))
```


## Announcements

- Student hours:
  - Tuesdays - 14:30 - 15:30 at JCMB 2257
  - Wednesdays - 14:30 - 15:30 online at [bit.ly/ids-zoom](http://bit.ly/ids-zoom)
  - Until 26 Nov 2019
- Peer evaluations: round 2 due Tue 12 Nov at 17:00 (tomorrow)
- Project proposal revisions (optional): due today Mon 11 Nov at 17:00 (tomorrow)

---

## From last time

- Model selection follows Occam's Razor: among competing hypotheses that predict 
equally well, the one with the fewest assumptions should be selected.
- We use adjusted R-squared to compare the strength of fit of models (as opposed 
to R-squared) since adjusted R-squared applies a penalty for additional 
variables included in the model, and only goes up if the predictive contribition 
of an additional explanatory variable is higher than the penalty it brings along.

---

class: center, middle

# Model selection

---

## Backwards elimination

- Start with **full** model (including all candidate explanatory variables and 
all candidate interactions)
- Remove one variable at a time, record the adjusted R-squared for each of 
the resulting models, and selected the model with the highest adjusted 
R-squared
- Continue until adjusted R-squared does not increase

---

## Forward selection

- Start with **empty** model
- Add one variable (or interaction effect) at a time, record the adjusted R-squared
for each of the resulting models, and select the model with the highest adjusted 
R-squared
- Continue until adjusted R-squared does not increase

---

class: center, middle

# Predicting professor evaluation scores

---

## Data load & prep

```{r include=FALSE}
evals <- read_csv(here::here("csv/evals-mod.csv"))
```

```{r eval=FALSE}
evals <- read_csv("data/evals-mod.csv")
```

```{r}
evals <- evals %>% 
  mutate(bty_avg = rowMeans(select(., bty_f1lower:bty_m2upper)))
```

---

## Starting small: score ~ cls_did_eval + cls_students + cls_perc_eval

.question[
What percent of the variability in evaluation scores is explained by the model?
]

```{r}
full_model <- lm(score ~ cls_did_eval + cls_students + cls_perc_eval, 
                 data = evals)
glance(full_model)$r.squared
glance(full_model)$adj.r.squared
```

---

```{r message=FALSE, cache=TRUE, fig.height=3,fig.width=7}
library(GGally)
evals %>%
  select(score, cls_did_eval, cls_students, cls_perc_eval) %>%
  ggpairs()
```

---

.question[
Suppose we definitely want to keep `cls_did_eval` in the model. Which of the 
other two variables (`cls_students` or `cls_perc_eval`) is least likely to 
be effective in increasing the model's predictive power?
]

```{r echo=FALSE,message=FALSE, cache=TRUE, fig.height=3,fig.width=7}
library(GGally)
evals %>%
  select(score, cls_did_eval, cls_students, cls_perc_eval) %>%
  ggpairs()
```

---

## Full model

```{r}
full_model <- lm(score ~ cls_did_eval + cls_students + cls_perc_eval, 
                 data = evals)
glance(full_model)$adj.r.squared
```

---

## Step 1:

.midi[
```{r}
# Remove cls_did_eval
s1_stu_perc <- lm(score ~ cls_students + cls_perc_eval, data = evals)
glance(s1_stu_perc)$adj.r.squared
```
]

--

.midi[
```{r}
# Remove cls_students
s1_did_perc <- lm(score ~ cls_did_eval + cls_perc_eval, data = evals)
glance(s1_did_perc)$adj.r.squared
```
]

--

.midi[
```{r}
# Remove cls_perc_eval
s1_did_stu <- lm(score ~ cls_did_eval + cls_students, data = evals)
glance(s1_did_stu)$adj.r.squared
```
]

---

.question[
Given the following adjusted R-squared values, which model should be selected 
in step 1 of backwards selection?
]

.pull-left[
.midi[
```{r}
# full model
glance(full_model)$adj.r.squared
# remove cls_did_eval
glance(s1_stu_perc)$adj.r.squared
```
]
]
.pull-right[
.midi[
```{r}
# remove cls_students
glance(s1_did_perc)$adj.r.squared
# remove cls_perc_eval
glance(s1_did_stu)$adj.r.squared
```
]
]

-- 

Removing `cls_students` (number of students in the class) resulted in the 
highest increase in adjusted R-squared, so the model with only `cls_did_eval` 
and `cls_perc_eval` (number and percentage of students who completed evaluations, 
respectively) should be selected.

---

## Step 2:

.midi[
```{r}
# Remove cls_did_eval
s2_perc <- lm(score ~ cls_perc_eval, data = evals)
glance(s2_perc)$adj.r.squared
```
]

--

.midi[
```{r}
# Remove cls_perc_eval
s2_did <- lm(score ~ cls_did_eval, data = evals)
glance(s2_did)$adj.r.squared
```
]

--

No further variables should be dropped since dropping any results in a decrease 
in adjusted R-squared. The model selected in the previous step should be the 
final model.

---

.question[
Given the following adjusted R-squared values, which model should be selected 
in step 2 of backwards selection?
]

.midi[
```{r}
glance(s1_did_perc)$adj.r.squared   # result of step 1
glance(s2_perc)$adj.r.squared       # remove cls_did_eval
glance(s2_did)$adj.r.squared        # remove cls_perc_eval
```
]

---

## A more realistic view: score ~ lots of variables

.question[
What percent of the variability in evaluation scores is explained by the model?
]

```{r}
full_model <- lm(score ~ rank + ethnicity + gender + language + 
                         age + cls_perc_eval + cls_did_eval + 
                         cls_students + cls_level + cls_profs + 
                         cls_credits + bty_avg, data = evals)
glance(full_model)$r.squared
glance(full_model)$adj.r.squared
```

---

## Step 1

.question[
Given that the adjusted R-squared of the full model was 
`r glance(full_model)$adj.r.squared %>% round(4)`, 
which of the following models should be selected in the first step of backwards 
selection?
]

.midi[
```{r echo=FALSE}
evals_sub <- evals %>%
  select(score, rank, ethnicity, gender, language, age, 
         cls_perc_eval, cls_did_eval, cls_students, cls_level, 
         cls_profs, cls_credits, bty_avg)

result <- data.frame(
  remove = rep(NA, ncol(evals_sub)-1),
  adj_r_sq = rep(NA, ncol(evals_sub)-1)
)

for(i in 1:(ncol(evals_sub)-1)){
  
  col_to_remove <- i+1
  result$remove[i] <- paste("Remove", names(evals_sub)[col_to_remove])
  
  evals_step1_mod <- evals_sub[,-col_to_remove]
  m <- lm(score ~ ., data = evals_step1_mod)
  result$adj_r_sq[i] <- glance(m)$adj.r.squared
}

result %>%
  arrange(desc(adj_r_sq)) %>%
  print()
```
]

--

Remove `cls_profs`

```{r include=FALSE}
step1 <- lm(score ~ rank + ethnicity + gender + language + 
                    age + cls_perc_eval + cls_did_eval + 
                    cls_students + cls_level + 
                    cls_credits + bty_avg, data = evals)
```


---

## Step 2

.question[
Given that the adjusted R-squared of the model selected in Step 1 was 
`r glance(step1)$adj.r.squared %>% round(4)`, 
which of the following models should be selected in the first step of backwards 
selection?
]

.midi[
```{r echo=FALSE}
evals_sub <- evals %>%
  select(score, rank, ethnicity, gender, language, age, 
         cls_perc_eval, cls_did_eval, cls_students, cls_level, 
         cls_credits, bty_avg)

result <- data.frame(
  remove = rep(NA, ncol(evals_sub)-1),
  adj_r_sq = rep(NA, ncol(evals_sub)-1)
)

for(i in 1:(ncol(evals_sub)-1)){
  
  col_to_remove <- i+1
  result$remove[i] <- paste("Remove", names(evals_sub)[col_to_remove])
  
  evals_step1_mod <- evals_sub[,-col_to_remove]
  m <- lm(score ~ ., data = evals_step1_mod)
  result$adj_r_sq[i] <- glance(m)$adj.r.squared
}

result %>%
  arrange(desc(adj_r_sq)) %>%
  print()
```
]

--

Remove `cls_level`

```{r include=FALSE}
step2 <- lm(score ~ rank + ethnicity + gender + language + 
                    age + cls_perc_eval + cls_did_eval + 
                    cls_students + 
                    cls_credits + bty_avg, data = evals)
```


---

## Step 3

.question[
Given that the adjusted R-squared of the model selected in Step 2 was 
`r glance(step2)$adj.r.squared %>% round(4)`, 
which of the following models should be selected in the first step of backwards 
selection?
]

.midi[
```{r echo=FALSE}
evals_sub <- evals %>%
  select(score, rank, ethnicity, gender, language, age, 
         cls_perc_eval, cls_did_eval, cls_students, cls_credits, bty_avg)

result <- data.frame(
  remove = rep(NA, ncol(evals_sub)-1),
  adj_r_sq = rep(NA, ncol(evals_sub)-1)
)

for(i in 1:(ncol(evals_sub)-1)){
  
  col_to_remove <- i+1
  result$remove[i] <- paste("Remove", names(evals_sub)[col_to_remove])
  
  evals_step1_mod <- evals_sub[,-col_to_remove]
  m <- lm(score ~ ., data = evals_step1_mod)
  result$adj_r_sq[i] <- glance(m)$adj.r.squared
}

result %>%
  arrange(desc(adj_r_sq)) %>%
  print()
```
]

--

Remove `cls_students`

```{r include=FALSE}
step3 <- lm(score ~ rank + ethnicity + gender + language + 
                    age + cls_perc_eval + cls_did_eval + 
                    cls_credits + bty_avg, data = evals)
```


---

## Step 4

.question[
Given that the adjusted R-squared of the model selected in Step 3 was 
`r glance(step3)$adj.r.squared %>% round(4)`, 
which of the following models should be selected in the first step of backwards 
selection?
]

.midi[
```{r echo=FALSE}
evals_sub <- evals %>%
  select(score, rank, ethnicity, gender, language, age, 
         cls_perc_eval, cls_did_eval, cls_credits, bty_avg)

result <- data.frame(
  remove = rep(NA, ncol(evals_sub)-1),
  adj_r_sq = rep(NA, ncol(evals_sub)-1)
)

for(i in 1:(ncol(evals_sub)-1)){
  
  col_to_remove <- i+1
  result$remove[i] <- paste("Remove", names(evals_sub)[col_to_remove])
  
  evals_step1_mod <- evals_sub[,-col_to_remove]
  m <- lm(score ~ ., data = evals_step1_mod)
  result$adj_r_sq[i] <- glance(m)$adj.r.squared
}

result %>%
  arrange(desc(adj_r_sq)) %>%
  print()
```
]

--

Remove `rank`

```{r include=FALSE}
step4 <- lm(score ~ ethnicity + gender + language + 
                    age + cls_perc_eval + cls_did_eval + 
                    cls_credits + bty_avg, data = evals)
```

---

## Step 5

.question[
Given that the adjusted R-squared of the model selected in Step 3 was 
`r glance(step4)$adj.r.squared %>% round(4)`, 
which of the following models should be selected in the first step of backwards 
selection?
]

.midi[
```{r echo=FALSE}
evals_sub <- evals %>%
  select(score, ethnicity, gender, language, age, 
         cls_perc_eval, cls_did_eval, cls_credits, bty_avg)

result <- data.frame(
  remove = rep(NA, ncol(evals_sub)-1),
  adj_r_sq = rep(NA, ncol(evals_sub)-1)
)

for(i in 1:(ncol(evals_sub)-1)){
  
  col_to_remove <- i+1
  result$remove[i] <- paste("Remove", names(evals_sub)[col_to_remove])
  
  evals_step1_mod <- evals_sub[,-col_to_remove]
  m <- lm(score ~ ., data = evals_step1_mod)
  result$adj_r_sq[i] <- glance(m)$adj.r.squared
}

result %>%
  arrange(desc(adj_r_sq)) %>%
  print()
```
]

--

None, stick with model from Step 4.

---

## Final model

```{r echo=FALSE}
tidy(step4)  %>% select(term, estimate)
```

---

## Model selection and interaction effects

Model selection for models including interaction effects must follow the 
following two principles:

- If an interaction is included in the model, the main effects of both of 
those variables must also be in the model.
- If a main effect is not in the model, then its interaction should not be 
in the model.

---

## Other model selection criteria

- Adjusted R-squared is one model selection criterion
- There are others out there (many many others!), we'll discuss one more in 
this course, and you'll learn about others in future stats courses

---

## Akaike Information Criterion

$$ AIC = -2log(L) + 2k $$

- $L$: likelihood	of the	model
    - Likelihood of seeing these data	given	the estimated model parameters
    - Won't go into calculating it in this course (but you will in future courses)
- Used for model selection, lower the better
    - Value is not informative on its own
- Applies	a	penalty	for	number of parameters in the	model, $k$
    - Different penalty than adjusted $R^2$ but similar idea

```{r aic-full-model}
glance(full_model)$AIC
```

---

## Model selection -- a little faster

`step()` function selects a model by AIC:

.midi[
```{r results="hide"}
selected_model <- step(full_model, direction = "backward")
```

```{r}
tidy(selected_model) %>% select(term, estimate)
```
]

---

## AIC comparison

```{r}
glance(full_model)$AIC
```

```{r}
glance(selected_model)$AIC
```

---

## Parsimony

.pull-left[
.question[
Take a look at the variables in the full and the selected model. Can you guess
why some of them may have been dropped? Remember: We like parsimonous models.
]
]
.pull-right[
.small[
| variable     | selected    |
| ------------ | :----------:|
| rank         |             |
| ethnicity    | x           |
| gender       | x           |
| language     | x           |
| age          | x           |
| cls_perc_eval| x           |
| cls_did_eval |             |
| cls_students |             |
| cls_level    |             |
| cls_profs    |             |
| cls_credits  | x           |
| bty_avg      | x           |
]
]

---

## Interpretation

.question[
Interpret the slope of `bty_avg` and `gender` in the selected model.
]

.midi[
```{r echo=FALSE}
tidy(selected_model) %>% select(term, estimate)
```
]

--
.midi[
- All else held constant, for each additional point in beauty score, the 
evaluation score of the professor is predicted to be higher, on average, 
by 0.06 points.
]

--
.midi[
- All else held constant, male professors are predicted to score higher on their 
evaluation score than female professors by 0.185 points.
]

---

class: center, middle

# Model validation

---

## Overfitting

- The data we are using to construct our models come from a larger population.
- Ultimately we want our model to tell us how the population works, not just the sample we have.
- If the model we fit is too tailored to our sample, it might not perform as well with the remaining population. This means the model is "overfitting" our data.
- We measure this using **model validation** techniques.
- Note: Overfitting is not a huge concern with linear models with low level 
interactions, however it can be with more complex and flexible models. The 
following is just an example of model validation, even though we're using it 
in a scenario where the concern for overfitting is not high.

---

## Model validation

- One commonly used model validation technique is to partition your data 
into training and testing set
- That is, fit the model on the training data
- And test it on the testing data
- Evaluate model performance using $RMSE$, root-mean squared error

$$ RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}} $$

.question[
Do you think we should prefer low or high RMSE?
]

---

## Random sampling and reproducibility

Gotta set a seed!

```{r}
set.seed(3518)
```

- We will be taking random samples, but we want the analysis to be 
reproducible (across different people running the sama analysis as well as 
for the same person running the analysis at different times)
- So we need to tell R where to start the (pseudo) random number generation



---

## Cross validation

More specifically, **k-fold cross validation**:

- Split your data into k folds
- Use 1 fold for testing and the remaining (k - 1) folds for training
- Repeat k times

---

## Aside -- the modulo operator

```{r}
9 %% 5
```

--

.pull-left[
```{r echo=FALSE}
df <- tibble(obs = 1:8, fold = as.integer(c(1,2,3,4,5,1,2,3)))
df
```
]

--

.pull-right[
.midi[
```{r}
(1:8) %% 5
((1:8) - 1) %% 5
(((1:8) - 1) %% 5) + 1
```
]
]

---

## Prepping your data for 5-fold CV

```{r}
evals_cv <- evals %>%
  sample_n(nrow(evals)) %>%               # scramble rows
  rowid_to_column() %>%                   # add id column
  mutate( fold = ((rowid - 1) %% 5) + 1 ) # add fold column

evals_cv %>% 
  count(fold)
```

---

## CV 1

```{r}
test_fold <- 1
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% filter(fold != test_fold)
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
(rmse_test1 <- rmse(mod, test))
```

---

## RMSE on training vs. testing

.question[
Would you expect the RMSE to be higher for your training data or your testing data? Why?
]

---

## RMSE on training vs. testing

RMSE for testing:
.small[
```{r}
(rmse_test1 <- rmse(mod, test))
```
]

RMSE for training:
.small[
```{r}
(rmse_train1 <- rmse(mod, train))
```
]

---

## CV 2

```{r}
test_fold <- 2
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% filter(fold != test_fold)
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test2 <- rmse(mod, test))
(rmse_train2 <- rmse(mod, train))
```

---

## CV 3

```{r}
test_fold <- 3
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% filter(fold != test_fold)
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test3 <- rmse(mod, test))
(rmse_train3 <- rmse(mod, train))
```

---

## CV 4

```{r}
test_fold <- 4
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% filter(fold != test_fold)
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test4 <- rmse(mod, test))
(rmse_train4 <- rmse(mod, train))
```

---

## CV 5

```{r}
test_fold <- 5
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% filter(fold != test_fold)
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test5 <- rmse(mod, test))
(rmse_train5 <- rmse(mod, train))
```

---

## Putting it altogether

```{r echo=FALSE}
rmse_evals <- tibble(
  test_fold  = 1:5,
  rmse_train = c(rmse_train1, rmse_train2, rmse_train3, rmse_train4, rmse_train5),
  rmse_test  = c(rmse_test1, rmse_test2, rmse_test3, rmse_test4, rmse_test5)
)

ggplot(data = rmse_evals, mapping = aes(x = test_fold, y = rmse_test)) +
  geom_point() +
  geom_line() +
  ylim(0, 1)
```

---

## How does RMSE compare to y?

- `score` summary stats:

```{r echo=FALSE}
evals %>%
  summarise(min = min(score), max = max(score), 
            mean = mean(score), med = median(score),
            sd = sd(score), IQR = IQR(score))
```

- `rmse_test` summary stats:

```{r echo=FALSE}
rmse_evals %>%
  summarise(min = min(rmse_test), max = max(rmse_test), 
            mean = mean(rmse_test), med = median(rmse_test),
            sd = sd(rmse_test), IQR = IQR(rmse_test))
```

---

class: center, middle

# Prediction

---

## New observation

To make a prediction for a new observation we need to create a data frame with that observation.

.question[
Suppose we want to make a prediction for a 37 year old white woman professor who received her education at an English speaking country and who teaches a multi credit course. 80% of her classes tend to fill out evaluations, and she's average looiking (beauty score = 2.5).

The following won't work. Why? How would you correct it?
]

```{r}
new_prof <- data_frame(ethnicity = "white",
                       sex = "woman",
                       language = "English",
                       age = 35, 
                       cls_perc_eval = 0.80,
                       cls_credits = "multi-credit",
                       bty_avg = 2.5)
```

---

## New observation, corrected

```{r}
new_prof <- data_frame(ethnicity = "not minority",
                       gender = "female",
                       language = "english",
                       age = 35, 
                       cls_perc_eval = 0.80,
                       cls_credits = "multi credit",
                       bty_avg = 2.5)
```

---

## Prediction

```{r}
predict(selected_model, newdata = new_prof)
```

---

## Uncertainty around prediction

Prediction interval around $\hat{y}$ for new data (score for a prof with given characteristics):

```{r}
predict(selected_model, newdata = new_prof, interval = "prediction")
```
