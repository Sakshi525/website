<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Text analysis   ðŸ“ƒ</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr.Â Ã‡etinkaya-Rundel" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Text analysis <br> ðŸ“ƒ
### Dr.Â Ã‡etinkaya-Rundel

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
Dr. Mine Ã‡etinkaya-Rundel -
&lt;a href="https://introds.org" target="_blank"&gt;introds.org
&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---



## Announcements

- This week's recap email will have instructions on checking/confirming your marks so far on Learn
- Don't forget to turn in peer feedback for projects (for your team and other teams) by 17:00 today
- HW 10 due today: You can use lower number of bootstrap samples, e.g. 1000.
- OQ 10 due Friday: Last item due!

---

## Piazza awards!

- 240 contributions; 71 days online: ___
- 62 contributions; 42 days online: ___
- 49 contributions; 46 days online: ___
- 18 contributions; 44 days online: ___

---

## Stickers!

Watch your email to find out when they'll be ready to pick up at my office. You can also stop by next term!

.pull-left[
![](img/logo-hex.png)
]
.pull-right[
![](img/github-stickers.png)
]

---

## Save the date!

.pull-left[
![](img/df-edi-logo.png)
]
.pull-right[
### ASA DataFest @ EDI  
&lt;br&gt;
Weekend-long data analysis hackathon, focusing on exploration, visualization, modeling, and insights  
&lt;br&gt;
March 13 - 15, 2020  
Bayes Centre  
[bit.ly/datafest-edi](http://bit.ly/datafest-edi)  
.small[*Sign-up opens soon!*]
]


---

class: center, middle

# Tidytext analysis

---

## Packages

In addition to `tidyverse` we will be using four other packages today


```r
library(tidytext)
library(genius)
library(wordcloud)
library(DT)
```

---

## Tidytext

- Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.
- Learn more at https://www.tidytextmining.com/.

---

## What is tidy text?


```r
text &lt;- c("Take me out tonight",
          "Where there's music and there's people",
          "And they're young and alive",
          "Driving in your car",
          "I never never want to go home",
          "Because I haven't got one",
          "Anymore")

text
```

```
## [1] "Take me out tonight"                   
## [2] "Where there's music and there's people"
## [3] "And they're young and alive"           
## [4] "Driving in your car"                   
## [5] "I never never want to go home"         
## [6] "Because I haven't got one"             
## [7] "Anymore"
```

---

## What is tidy text?


```r
text_df &lt;- tibble(line = 1:7, text = text)

text_df
```

```
## # A tibble: 7 x 2
##    line text                                  
##   &lt;int&gt; &lt;chr&gt;                                 
## 1     1 Take me out tonight                   
## 2     2 Where there's music and there's people
## 3     3 And they're young and alive           
## 4     4 Driving in your car                   
## 5     5 I never never want to go home         
## 6     6 Because I haven't got one             
## 7     7 Anymore
```

---

## What is tidy text?


```r
text_df %&gt;%
  unnest_tokens(word, text)
```

```
## # A tibble: 32 x 2
##     line word   
##    &lt;int&gt; &lt;chr&gt;  
##  1     1 take   
##  2     1 me     
##  3     1 out    
##  4     1 tonight
##  5     2 where  
##  6     2 there's
##  7     2 music  
##  8     2 and    
##  9     2 there's
## 10     2 people 
## # â€¦ with 22 more rows
```

---

class: center, middle

# What are you listening to?

---

## From the "Getting to know you" survey

&gt; "What are your 3 - 5 most favorite songs right now?"

.midi[

```r
listening &lt;- read_csv("data/listening.csv")
listening
```

```
## # A tibble: 104 x 1
##    songs                                                                   
##    &lt;chr&gt;                                                                   
##  1 Gamma Knife - King Gizzard and the Lizard Wizard; Self Immolate - King â€¦
##  2 I dont listen to much music                                             
##  3 Mess by Ed Sheeran, Take me back to london by Ed Sheeran and Sounds of â€¦
##  4 Hate Me (Sometimes) - Stand Atlantic; Edge of Seventeen - Stevie Nicks;â€¦
##  5 whistle, gogobebe, sassy me                                             
##  6 Shofukan, Think twice, Padiddle                                         
##  7 Groundislava - Feel the Heat (Indecorum Remix), Nominal - Everyday Everâ€¦
##  8 Loving you - passion the musical, Senorita - Shawn Mendes and Camilla Câ€¦
##  9 lay it down slow - spiritualised, dead boys - Sam Fender, figure it outâ€¦
## 10 Don't Stop Me Now (Queen), Finale (Toby Fox), Machine in the Walls (Mudâ€¦
## # â€¦ with 94 more rows
```
]

---

## Looking for commonalities

.midi[

```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 786 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 the      56
##  2 by       23
##  3 to       20
##  4 and      19
##  5 i        19
##  6 you      15
##  7 of       13
##  8 a        11
##  9 in       11
## 10 me       10
## # â€¦ with 776 more rows
```
]

---

## Stop words

- In computing, stop words are words which are filtered out before or after processing of natural language data (text).
- They usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.

---

## English stop words


```r
get_stopwords()
```

```
## # A tibble: 175 x 2
##    word      lexicon 
##    &lt;chr&gt;     &lt;chr&gt;   
##  1 i         snowball
##  2 me        snowball
##  3 my        snowball
##  4 myself    snowball
##  5 we        snowball
##  6 our       snowball
##  7 ours      snowball
##  8 ourselves snowball
##  9 you       snowball
## 10 your      snowball
## # â€¦ with 165 more rows
```

---

## Spanish stop words


```r
get_stopwords(language = "es")
```

```
## # A tibble: 308 x 2
##    word  lexicon 
##    &lt;chr&gt; &lt;chr&gt;   
##  1 de    snowball
##  2 la    snowball
##  3 que   snowball
##  4 el    snowball
##  5 en    snowball
##  6 y     snowball
##  7 a     snowball
##  8 los   snowball
##  9 del   snowball
## 10 se    snowball
## # â€¦ with 298 more rows
```

---

## Various lexicons

See `?get_stopwords` for more info.

.midi[

```r
get_stopwords(source = "smart")
```

```
## # A tibble: 571 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           smart  
##  2 a's         smart  
##  3 able        smart  
##  4 about       smart  
##  5 above       smart  
##  6 according   smart  
##  7 accordingly smart  
##  8 across      smart  
##  9 actually    smart  
## 10 after       smart  
## # â€¦ with 561 more rows
```
]

---

## Back to: Looking for commonalities

.small[

```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
* anti_join(stop_words) %&gt;%
* filter(!(word %in% c("1", "2", "3", "4", "5"))) %&gt;%
  count(word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 640 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 ed          7
##  2 queen       7
##  3 sheeran     7
##  4 love        6
##  5 bad         5
##  6 time        5
##  7 1975        4
##  8 dog         4
##  9 king        4
## 10 life        4
## # â€¦ with 630 more rows
```
]

---

## Top 20 common words in songs

.pull-left[
.small[

```r
top20_songs &lt;- listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  anti_join(stop_words) %&gt;%
  filter(
    !(word %in% c("1", "2", "3", "4", "5"))
    ) %&gt;%
  count(word) %&gt;%
  top_n(20)
```
]
]
.pull-right[
.midi[

```r
top20_songs %&gt;%
  arrange(desc(n))
```

```
## # A tibble: 41 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 ed          7
##  2 queen       7
##  3 sheeran     7
##  4 love        6
##  5 bad         5
##  6 time        5
##  7 1975        4
##  8 dog         4
##  9 king        4
## 10 life        4
## # â€¦ with 31 more rows
```
]
]
---

## Visualizing commonalities: bar chart

.midi[
&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-14-1.png" width="4200" /&gt;
]

---

... the code


```r
ggplot(top20_songs, aes(x = fct_reorder(word, n), y = n)) +
  geom_col() +
  labs(x = "Common words", y = "Count") +
  coord_flip()
```


---

## Visualizing commonalities: wordcloud

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-16-1.png" width="1500" /&gt;

---

... and the code


```r
set.seed(1234)
wordcloud(words = top20_songs$word, 
          freq = top20_songs$n, 
          colors = brewer.pal(5,"Blues"),
          random.order = FALSE)
```

---

## Ok, so people like Ed Sheeran!


```r
str_subset(listening$songs, "Sheeran")
```

```
## [1] "Mess by Ed Sheeran, Take me back to london by Ed Sheeran and Sounds of the Skeng by Stormzy"                
## [2] "Ed Sheeran- I don't care, beautiful people, don't"                                                          
## [3] "Truth Hurts by Lizzo , Wetsuit by The Vaccines , Beautiful People by Ed Sheeran"                            
## [4] "Sounds of the Skeng - Stormzy, Venom - Eminem, Take me back to london - Ed Sheeran, I see fire - Ed Sheeran"
```

---

## But I had to ask...

--

What is 1975?

--


```r
str_subset(listening$songs, "1975")
```

```
## [1] "Hate Me (Sometimes) - Stand Atlantic; Edge of Seventeen - Stevie Nicks; It's Not Living (If It's Not With You) - The 1975; People - The 1975; Hypersonic Missiles - Sam Fender"
## [2] "Chocolate by the 1975, sanctuary by Joji, A young understating by Sundara Karma"                                                                                               
## [3] "Lauv - I'm lonely, kwassa - good life, the 1975 - sincerity is scary"
```

---

class: center, middle

# Analyzing lyrics of one artist

---

## Let's get more data

We'll use the **genius** package to get song lyric data from [Genius](https://genius.com/).

- `genius_album()`: download lyrics for an entire album
- `add_genius()`: download lyrics for multiple albums

---

## Ed's most recent-ish albums


```r
artist_albums &lt;- tribble(
  ~artist,      ~album,
  "Ed Sheeran", "No.6 Collaborations Project",
  "Ed Sheeran", "Divide",
  "Ed Sheeran", "Multiply",
  "Ed Sheeran", "Plus",
)

sheeran &lt;- artist_albums %&gt;%
  add_genius(artist, album)
```

---

## Songs in the four albums

.small[
<div id="htmlwidget-4c10c17d05b981b4cf0a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-4c10c17d05b981b4cf0a">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73"],["No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","No.6 Collaborations Project","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Divide","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Multiply","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus","Plus"],["Beautiful People (Ft.Â Khalid)","South of the Border (Ft.Â CamilaÂ Cabello &amp; CardiÂ B)","Cross Me (Ft.Â ChanceÂ the Rapper &amp; PnBÂ Rock)","Take Me Back to London (Ft.Â Stormzy)","Best Part of Me (Ft.Â YEBBA)","I Don't Care byÂ EdÂ Sheeran &amp; Justin Bieber","Antisocial byÂ EdÂ Sheeran &amp; Travis Scott","Remember the Name (Ft.Â 50Â Cent &amp; Eminem)","Feels (Ft.Â JÂ Hus &amp; YoungÂ Thug)","Put It All on Me (Ft.Â EllaÂ Mai)","Nothing on You (Ft.Â Dave &amp; PauloÂ Londra)","I Don't Want Your Money (Ft.Â H.E.R.)","1000 Nights (Ft.Â AÂ Boogie Wit da Hoodie &amp; MeekÂ Mill)","Way to Break My Heart (Ft.Â Skrillex)","BLOW byÂ EdÂ Sheeran, Chris Stapleton &amp; Bruno Mars","Eraser","Castle on the Hill","Dive","Shape of You","Perfect","Galway Girl","Happier","New Man","Hearts Don't Break Around Here","What Do I Know?","How Would You Feel (Paean)","Supermarket Flowers","Barcelona","Bibia Be Ye Ye","Nancy Mulligan","Save Myself","Perfect Duet byÂ EdÂ Sheeran &amp; Beyonce","One","I'm a Mess","Sing! (Ft.Â PharrellÂ Williams)","Don't","Nina","Photograph","Bloodstream","Tenerife Sea","Runaway","The Man","Thinking Out Loud","Afire Love","Take It Back","Shirtsleeves","Even My Dad Does Sometimes","I See Fire","All of the Stars","English Rose","Touch and Go","New York","Make It Rain","Lay It All on Me byÂ Rudimental (Ft.Â EdÂ Sheeran)","The A Team","Drunk","U.N.I.","Grade 8","Wake Me Up","Small Bump","This","The City","Lego House","You Need Me, I Don't Need You","Kiss Me","Give Me Love","The Parting Glass","Autumn Leaves","Little Bird","Gold Rush","Sunburn","Sofa","Homeless"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>album<\/th>\n      <th>track_title<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"p","order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
]

---

## How long are Ed Sheeran's songs?

Length measured by number of lines


```r
sheeran %&gt;%
  count(track_title, sort = TRUE)
```

```
## # A tibble: 73 x 2
##    track_title                                            n
##    &lt;chr&gt;                                              &lt;int&gt;
##  1 Take It Back                                         101
##  2 The Man                                               96
##  3 Cross Me (Ft.Â ChanceÂ the Rapper &amp; PnBÂ Rock)           95
##  4 You Need Me, I Don't Need You                         94
##  5 Shape of You                                          91
##  6 Nina                                                  89
##  7 Remember the Name (Ft.Â 50Â Cent &amp; Eminem)              77
##  8 South of the Border (Ft.Â CamilaÂ Cabello &amp; CardiÂ B)    77
##  9 Bloodstream                                           76
## 10 Take Me Back to London (Ft.Â Stormzy)                  75
## # â€¦ with 63 more rows
```

---

## Tidy up your lyrics!


```r
sheeran_lyrics &lt;- sheeran %&gt;%
  unnest_tokens(word, lyric)

sheeran_lyrics
```

```
## # A tibble: 30,023 x 6
##    artist    album               track_title           track_n  line word  
##    &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;                   &lt;int&gt; &lt;int&gt; &lt;chr&gt; 
##  1 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 we    
##  2 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 are   
##  3 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 we    
##  4 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 are   
##  5 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 we    
##  6 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     1 are   
##  7 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     2 l.a   
##  8 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     2 on    
##  9 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     2 a     
## 10 Ed Sheerâ€¦ No.6 Collaborationâ€¦ Beautiful People (Ftâ€¦       1     2 saturâ€¦
## # â€¦ with 30,013 more rows
```

---

## What are the most common words?


```r
sheeran_lyrics %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 2,830 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 i      1161
##  2 you    1013
##  3 the     992
##  4 and     886
##  5 my      823
##  6 me      689
##  7 to      556
##  8 in      517
##  9 a       500
## 10 on      408
## # â€¦ with 2,820 more rows
```

---

## What a romantic!

.midi[

```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 2,413 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 love    363
##  2 baby    130
##  3 yeah    130
##  4 wanna    97
##  5 home     96
##  6 time     95
##  7 heart    76
##  8 ye       75
##  9 feel     72
## 10 eyes     69
## # â€¦ with 2,403 more rows
```
]

---

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-26-1.png" width="3000" /&gt;

---

... and the code


```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word)%&gt;%
  top_n(20) %&gt;%
  ggplot(aes(fct_reorder(word, n), n)) +
    geom_col() +
    labs(title = "Frequency of Ed Sheeran's lyrics",
         subtitle = "`Love` tops the chart",
         y = "",
         x = "") +
    coord_flip()
```

---

class: center, middle

# Sentiment analysis

---

## Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a combination of its individual words 
- and the sentiment content of the whole text as the sum of the sentiment content of the individual words

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("afinn")
```

```
## # A tibble: 2,477 x 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # â€¦ with 2,467 more rows
```
]
.pull-right[

```r
get_sentiments("bing") 
```

```
## # A tibble: 6,786 x 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 2-faces     negative 
##  2 abnormal    negative 
##  3 abolish     negative 
##  4 abominable  negative 
##  5 abominably  negative 
##  6 abominate   negative 
##  7 abomination negative 
##  8 abort       negative 
##  9 aborted     negative 
## 10 aborts      negative 
## # â€¦ with 6,776 more rows
```
]

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("nrc")
```

```
## # A tibble: 13,901 x 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 abacus      trust    
##  2 abandon     fear     
##  3 abandon     negative 
##  4 abandon     sadness  
##  5 abandoned   anger    
##  6 abandoned   fear     
##  7 abandoned   negative 
##  8 abandoned   sadness  
##  9 abandonment anger    
## 10 abandonment fear     
## # â€¦ with 13,891 more rows
```
]
.pull-right[

```r
get_sentiments("loughran") 
```

```
## # A tibble: 4,150 x 2
##    word         sentiment
##    &lt;chr&gt;        &lt;chr&gt;    
##  1 abandon      negative 
##  2 abandoned    negative 
##  3 abandoning   negative 
##  4 abandonment  negative 
##  5 abandonments negative 
##  6 abandons     negative 
##  7 abdicated    negative 
##  8 abdicates    negative 
##  9 abdicating   negative 
## 10 abdication   negative 
## # â€¦ with 4,140 more rows
```
]

---

class: middle

## Categorizing sentiments

---

## Sentiments in Sheeran's lyrics

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 359 x 3
##    sentiment word          n
##    &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
##  1 positive  love        363
##  2 positive  like        202
##  3 positive  right        46
##  4 positive  well         33
##  5 positive  darling      31
##  6 positive  lover        30
##  7 positive  loved        29
##  8 negative  fall         25
##  9 positive  beautiful    23
## 10 negative  cold         22
## # â€¦ with 349 more rows
```
]

---

class: middle

**Goal:** Find the top 10 most common words with positive and negative sentiments.

---

### Step 1: Top 10 words for each sentiment

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) 
```

```
## # A tibble: 20 x 3
## # Groups:   sentiment [2]
##    sentiment word          n
##    &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
##  1 negative  bad          16
##  2 negative  break        21
##  3 negative  cold         22
##  4 negative  drunk        19
##  5 negative  fall         25
##  6 negative  falling      22
##  7 negative  hurting      16
##  8 negative  miss         20
##  9 negative  pain         19
## 10 negative  shit         20
## 11 positive  beautiful    23
## 12 positive  better       19
## 13 positive  darling      31
## 14 positive  free         22
## 15 positive  like        202
## 16 positive  love        363
## 17 positive  loved        29
## 18 positive  lover        30
## 19 positive  right        46
## 20 positive  well         33
```
]

---

### Step 2: `ungroup()`

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

```
## # A tibble: 20 x 3
##    sentiment word          n
##    &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
##  1 negative  bad          16
##  2 negative  break        21
##  3 negative  cold         22
##  4 negative  drunk        19
##  5 negative  fall         25
##  6 negative  falling      22
##  7 negative  hurting      16
##  8 negative  miss         20
##  9 negative  pain         19
## 10 negative  shit         20
## 11 positive  beautiful    23
## 12 positive  better       19
## 13 positive  darling      31
## 14 positive  free         22
## 15 positive  like        202
## 16 positive  love        363
## 17 positive  loved        29
## 18 positive  lover        30
## 19 positive  right        46
## 20 positive  well         33
```
]

---

### Step 3: Save the result


```r
sheeran_top10 &lt;- sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

---

class: middle

**Goal:** Visualize the top 10 most common words with positive and negative sentiments.

---

### Step 1: Create a bar chart


```r
sheeran_top10 %&gt;%
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-36-1.png" width="3000" /&gt;

---

### Step 2: Order bars by frequency


```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-37-1.png" width="3000" /&gt;

---

### Step 3: Facet by sentiment


```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment)
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-38-1.png" width="3000" /&gt;

---

### Step 4: Free the scales!


```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free")
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-39-1.png" width="3000" /&gt;

---

### Step 4: Flip the coordinates


```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-40-1.png" width="3000" /&gt;

---

### Step 5: Clean up labels

.small[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Ed Sheeran's lyrics", x = "", y = "")
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-41-1.png" width="3000" /&gt;
]

---

### Step 6: Remove redundant info

.small[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Ed Sheeran's lyrics", x = "", y = "") +
  guides(fill = FALSE) 
```

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-42-1.png" width="3000" /&gt;
]

---

class: middle

## Scoring sentiments

---

## Assign a sentiment score


```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) 
```

```
## Joining, by = "word"Joining, by = "word"
```

```
## # A tibble: 9,186 x 7
##    artist   album           track_title        track_n  line word     value
##    &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     2 l.a         NA
##  2 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     2 saturday    NA
##  3 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     2 night       NA
##  4 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     2 summer      NA
##  5 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     3 sundown     NA
##  6 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     4 lamborgâ€¦    NA
##  7 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     4 rented      NA
##  8 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     4 hummers     NA
##  9 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     5 party's     NA
## 10 Ed Sheeâ€¦ No.6 Collaboraâ€¦ Beautiful People â€¦       1     5 headin      NA
## # â€¦ with 9,176 more rows
```

---


```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) %&gt;%
  filter(!is.na(value)) %&gt;%
  group_by(album) %&gt;%
  summarise(total_sentiment = sum(value)) %&gt;%
  arrange(total_sentiment)
```

```
## # A tibble: 4 x 2
##   album                       total_sentiment
##   &lt;chr&gt;                                 &lt;dbl&gt;
## 1 No.6 Collaborations Project             102
## 2 Multiply                                112
## 3 Plus                                    218
## 4 Divide                                  378
```

---

&lt;img src="w11-d2-text-analysis_files/figure-html/unnamed-chunk-45-1.png" width="4800" /&gt;

---

## Acknowledgements

- Julia Silge: https://github.com/juliasilge/tidytext-tutorial
- Julia Silge and David Robinson: https://www.tidytextmining.com/
- Josiah Parry: https://github.com/JosiahParry/genius
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
